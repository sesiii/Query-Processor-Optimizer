Title
Query Processor and Optimizer
Abstract
Efficient query processing is a cornerstone of database management systems, balancing
performance and resource utilization. This project aims to develop a query processor and
optimizer that parses SQL-like queries, constructs computation graphs, generates execu-
tion plans, and estimates operational costs. Leveraging Bison and Flex, we will build
a parser to translate queries into a structured format. The system will then produce
optimized execution plans, inspired by PostgreSQL and SQLite, incorporating basic op-
timizations such as selection pushdown and join reordering, as outlined in Silberschatz et
al.’s ”Database System Concepts.” Cost estimation will rely on simplified table statistics
and operation complexity. By integrating systems-level tools with database theory, this
project will deliver a functional prototype that demonstrates query transformation and
cost-aware optimization within a constrained timeline.
Weekly Work Plan
Week 1: March 17–23, 2025 (Proposal Submission)
• Finalize project scope and objectives.
• Research query processing and optimization (Silberschatz Ch. 13–14).
• Draft and submit proposal.
• Set up Bison and Flex for SQL-like grammar.
• Implement parser to generate abstract syntax tree (AST).
1
• Convert AST to basic computation graph for simple queries.
Week 2: March 24–30, 2025 (Parser and Computation Graph)
• Generate initial execution plans (e.g., relational algebra).
• Develop basic cost estimation model (I/O and CPU costs).
• Test with single-table and join queries.
Week 3: March 31–April 6, 2025 (Execution Plan and Cost Estimation)
• Implement selection pushdown and join reordering optimizations.
• Compare costs of optimized vs. unoptimized plans.
• Test with diverse queries and debug.
Week 4: April 14–15, 2025 (Finalization and Submission)
• Polish code and validate results.
• Write final report and prepare demo.
• Submit by April 15, 2025.




we need to implemented cost based metrics to see if the optimization is atually effective(say it disk reads or simple no of i/o s), we need to show before and afteer optimization metrics that the optimization is correct. we are only implementding the [arser optimization for 3-4 queries, therefore we need to have stats about them in stats.h, also here is a rough idead about how we need to show cost , assume 2 tables on whivh we are operating, have total 10000 tupes, out of which only 100 match, so join operation will effective reduce to 100 tuples after selection/projection piushdowm, similarly, we need to have metric to evelaute the optimizations





/////////////////////////////////
Abstract
Efficient query processing is a cornerstone of database management systems, balancing
performance and resource utilization. This project aims to develop a query processor and
optimizer that parses SQL-like queries, constructs computation graphs, generates execu-
tion plans, and estimates operational costs. Leveraging Bison and Flex, we will build
a parser to translate queries into a structured format. The system will then produce
optimized execution plans, inspired by PostgreSQL and SQLite, incorporating basic op-
timizations such as selection pushdown and join reordering, as outlined in Silberschatz et
al.’s ”Database System Concepts.” Cost estimation will rely on simplified table statistics
and operation complexity. By integrating systems-level tools with database theory, this
project will deliver a functional prototype that demonstrates query transformation and
cost-aware optimization within a constrained timeline.
Weekly Work Plan
Week 1: March 17–23, 2025 (Proposal Submission)
• Finalize project scope and objectives.
• Research query processing and optimization (Silberschatz Ch. 13–14).
• Draft and submit proposal.
• Set up Bison and Flex for SQL-like grammar.
• Implement parser to generate abstract syntax tree (AST).
1
• Convert AST to basic computation graph for simple queries.
Week 2: March 24–30, 2025 (Parser and Computation Graph)
• Generate initial execution plans (e.g., relational algebra).
• Develop basic cost estimation model (I/O and CPU costs).
• Test with single-table and join queries.
Week 3: March 31–April 6, 2025 (Execution Plan and Cost Estimation)
• Implement selection pushdown and join reordering optimizations.
• Compare costs of optimized vs. unoptimized plans.
• Test with diverse queries and debug.
Week 4: April 14–15, 2025 (Finalization and Submission)
• Polish code and validate results.
• Write final report and prepare demo.
• Submit by April 15, 2025.

pls go through the code, i have at a stage where a query is transformed into an ast, then execution plan is generated, then later an optimization is done, comparing of execustipon plans before and after optimizations.

i am attaching output for a query for your reference
dadi@RogStrix:~/Desktop/Query_Processor_Optimizer/code_1$ make
flex lexer.l
bison -d parser.y
gcc lex.yy.c parser.tab.c main.c stats.c optimizer.c -o query_processor -lm
./query_processor
Parsing query: SELECT employees.name, salaries.salary FROM employees JOIN salaries ON employees.emp_id = salaries.emp_id WHERE salaries.salary > 50000;

Original Abstract Syntax Tree:
π(employees.name,salaries.salary)
  σ(salaries.salary > 50000)
    table(employees)
      table(salaries)
      ⨝(employees.emp_id = salaries.emp_id)

Optimizing query...
Initializing statistics with hardcoded values
Statistics initialized for 4 tables

Original Execution Plan:
--- Original Plan ---
π(employees.name,salaries.salary) [size=8333, io=10000.0, cpu=3416.7]
  σ(salaries.salary > 50000) [size=8333, io=10000.0, cpu=3000.0]
    table(employees) [size=10000, io=10000.0, cpu=1000.0]
      table(salaries) [size=10000, io=10000.0, cpu=1000.0]
    ⨝(employees.emp_id = salaries.emp_id) [size=0, io=0.0, cpu=0.0]

Applying projection push-down...

Optimized Execution Plan:
--- Optimized Plan ---
σ(salaries.salary > 50000) [size=8333, io=10000.0, cpu=6500.0]
  π(employees.name,salaries.salary) [size=10000, io=10000.0, cpu=4500.0]
    ⨝(employees.emp_id = salaries.emp_id) [size=10000, io=10000.0, cpu=4000.0]
      π(employees.name,employees.emp_id) [size=10000, io=5000.0, cpu=1500.0]
        table(employees) [size=10000, io=10000.0, cpu=1000.0]
      π(salaries.salary,salaries.emp_id) [size=10000, io=5000.0, cpu=1500.0]
        table(salaries) [size=10000, io=10000.0, cpu=1000.0]

Cost Comparison:
Metric          | Original      | Optimized     | Change
----------------|---------------|---------------|----------------
Result Size     | 8333          | 8333          | -0 (0.0%)
I/O Cost        | 10000.0       | 10000.0       | -0.0 (0.0%)
CPU Cost        | 3416.7        | 6500.0        | +3083.3 (90.2%)
rm -f lex.yy.c parser.tab.c parser.tab.h query_processor



but now, i am implementing an optimization as per my wish in the optimizer.c file, youll see in the initial part/

what i want is, implenting both optimizations, compare both optimizations, and then showing the best optimization wrt cost

struct data{
string tname;
int ncols;
int ntups;
vector<string> col_name;
vector<vector<string>>tuples;
}Data;

well consider the cost this way, becuse this seems to be the best way to estimate costs, in selective pushdown, now of rows decrease and in projection pushdown, no of columns decrease, as compared to unoptimized plan, cost will be always less than no of rows* no of columns.
Well implement this as a cost metric, no other metric

also suggest anything you feel we can consider, as we are not working on a realdatabase, everything is in a separate file(metadata), so well also get to change it as per our needs directly in the file



π(employees.name,departments.dept_name) [rows=500, cols=2, cost=1000.0]
  σ(departments.dept_name = Engineering) [rows=500, cols=4, cost=2000.0]
    ⨝(employees.dept_id = departments.dept_id) [rows=0, cols=0, cost=0.0]
      table(employees) [rows=10000, cols=4, cost=40000.0]
      table(departments) [rows=20, cols=3, cost=60.0]


. Original Plan: Total Cost = 45883.5

Plan Structure:
text
π(employees.name,departments.dept_name) [rows=500, cols=2, cost=1000.0]
  σ(departments.dept_name = Engineering) [rows=500, cols=7, cost=3500.0]
    ⨝(employees.dept_id = departments.dept_id) [rows=10000, cols=7, cost=70000.0]
      table(employees) [rows=10000, cols=4, cost=40000.0]
      table(departments) [rows=20, cols=3, cost=60.0]

Breakup Table:
text
Node Type | Description                              | Node Cost | Cumulative Cost
----------|------------------------------------------|-----------|----------------
π         | employees.name,departments.dept_name     | 1000.0    | 45883.5
σ         | departments.dept_name = Engineering      | 3500.0    | 49413.0
⨝         | employees.dept_id = departments.dept_id  | 70000.0   | 47060.0
table     | employees                                | 40000.0   | 40000.0
table     | departments                              | 60.0      | 60.0

Math Explanation:
Let’s compute the Cumulative Cost using calculate_total_plan_cost bottom-up:

    Table (employees):
        Node Cost: result_size * num_columns = 10000 * 4 = 40000.0
        Cumulative Cost: For a table, calculate_total_plan_cost returns estimate_cost.cost = 40000.0
        Formula: return current.cost;
        Result: Cumulative Cost = 40000.0
    Table (departments):
        Node Cost: 20 * 3 = 60.0
        Cumulative Cost: 60.0
        Formula: return current.cost;
        Result: Cumulative Cost = 60.0
    Join (⨝):
        Node Cost: 10000 * 7 = 70000.0
        Selectivity: get_join_selectivity returns 0.05 (default).
        Child Costs:
            Left (employees): 40000.0
            Right (departments): 60.0
        Cumulative Cost:
            Formula: left_cost + right_cost + (result_size * num_columns * 0.1)
            Calculation: 40000.0 + 60.0 + (10000 * 7 * 0.1) = 40060.0 + 7000.0 = 47060.0
        Result: Cumulative Cost = 47060.0
    Selection (σ):
        Node Cost: 500 * 7 = 3500.0
        Selectivity: get_condition_selectivity returns 0.05.
        Child Cost (join): 47060.0
        Cumulative Cost:
            Formula: child_cost * (1.0 + selectivity)
            Calculation: 47060.0 * (1.0 + 0.05) = 47060.0 * 1.05 = 49413.0
        Result: Cumulative Cost = 49413.0
    Projection (π):
        Node Cost: 500 * 2 = 1000.0
        Child Cost (selection): 49413.0
        Column Ratio: num_columns / child_num_columns = 2 / 7 ≈ 0.2857
        Cumulative Cost:
            Formula: child_cost * (0.9 + 0.1 * column_ratio)
            Calculation: 49413.0 * (0.9 + 0.1 * 0.2857) = 49413.0 * (0.9 + 0.02857) = 49413.0 * 0.92857 ≈ 45883.5
        Result: Cumulative Cost = 45883.5

Total Cost: The Cumulative Cost of the root node (π) is 45883.5, which matches the Total Cost in the Cost Comparison table.

Why Node Costs Don’t Sum to Total:

    Node Costs (1000.0 + 3500.0 + 70000.0 + 40000.0 + 60.0 = 114560.0) are from estimate_cost and represent raw data volume.
    Cumulative Costs include:
        Selectivity (1.05 for σ, 0.05 for ⨝) adjusts costs to reflect filtering/joining efficiency.
        Column Ratio (0.2857 for π) reduces cost due to fewer output columns.
        Recursive aggregation means each node’s Cumulative Cost includes its children’s costs, not just its Node Cost.

Breakup Table Insight:

    The table shows the subtree cost at each node:
        π: Entire plan cost = 45883.5
        σ: Cost of σ + join + tables = 49413.0
        ⨝: Cost of join + tables = 47060.0
        Tables: Just their own costs (40000.0, 60.0).
    The Total Cost (45883.5) is the π node’s Cumulative Cost, as it’s the root.

2. Selection Pushdown Plan: Total Cost = 37526.4

Plan Structure:
text
π(employees.name,departments.dept_name) [rows=500, cols=2, cost=1000.0]
  ⨝(employees.dept_id = departments.dept_id) [rows=500, cols=7, cost=3500.0]
    table(employees) [rows=10000, cols=4, cost=40000.0]
    σ(departments.dept_name = Engineering) [rows=1, cols=3, cost=3.0]
      table(departments) [rows=20, cols=3, cost=60.0]

Breakup Table:
text
Node Type | Description                              | Node Cost | Cumulative Cost
----------|------------------------------------------|-----------|----------------
π         | employees.name,departments.dept_name     | 1000.0    | 37526.4
⨝         | employees.dept_id = departments.dept_id  | 3500.0    | 40413.0
table     | employees                                | 40000.0   | 40000.0
σ         | departments.dept_name = Engineering      | 3.0       | 63.0
table     | departments                              | 60.0      | 60.0

Math Explanation:

    Table (departments):
        Node Cost: 20 * 3 = 60.0
        Cumulative Cost: 60.0
        Result: Cumulative Cost = 60.0
    Selection (σ):
        Node Cost: 1 * 3 = 3.0
        Selectivity: 0.05
        Child Cost (departments): 60.0
        Cumulative Cost:
            Formula: child_cost * (1.0 + selectivity)
            Calculation: 60.0 * (1.0 + 0.05) = 60.0 * 1.05 = 63.0
        Result: Cumulative Cost = 63.0
    Table (employees):
        Node Cost: 10000 * 4 = 40000.0
        Cumulative Cost: 40000.0
        Result: Cumulative Cost = 40000.0
    Join (⨝):
        Node Cost: 500 * 7 = 3500.0
        Selectivity: 0.05
        Child Costs:
            Left (employees): 40000.0
            Right (σ): 63.0
        Cumulative Cost:
            Formula: left_cost + right_cost + (result_size * num_columns * 0.1)
            Calculation: 40000.0 + 63.0 + (500 * 7 * 0.1) = 40063.0 + 350.0 = 40413.0
        Result: Cumulative Cost = 40413.0
    Projection (π):
        Node Cost: 500 * 2 = 1000.0
        Child Cost (join): 40413.0
        Column Ratio: 2 / 7 ≈ 0.2857
        Cumulative Cost:
            Formula: child_cost * (0.9 + 0.1 * column_ratio)
            Calculation: 40413.0 * (0.9 + 0.1 * 0.2857) = 40413.0 * 0.92857 ≈ 37526.4
        Result: Cumulative Cost = 37526.4

Total Cost: The root node’s Cumulative Cost = 37526.4.

Why Lower Cost?:

    The selection (σ) is pushed down to departments, reducing its rows to 1 before the join.
    This makes the join process only 500 rows (vs. 10000 in Original), lowering the join’s cost (40413.0 vs. 47060.0).
    Node Costs (1000.0 + 3500.0 + 40000.0 + 3.0 + 60.0 = 44563.0) don’t sum to 37526.4 due to selectivity (1.05 for σ, 0.05 for ⨝) and column ratio (0.2857 for π).

Breakup Table Insight:

    π: Entire plan = 37526.4
    ⨝: Join + employees + σ subtree = 40413.0
    Table (employees): 40000.0
    σ: σ + departments = 63.0
    Table (departments): 60.0

3. Projection Pushdown Plan: Total Cost = 42060.9

Plan Structure:
text
σ(departments.dept_name = Engineering) [rows=500, cols=2, cost=1000.0]
  π(employees.name,departments.dept_name) [rows=10000, cols=2, cost=20000.0]
    ⨝(employees.dept_id = departments.dept_id) [rows=10000, cols=2, cost=20000.0]
      π(employees.name,employees.dept_id) [rows=10000, cols=2, cost=20000.0]
        table(employees) [rows=10000, cols=4, cost=40000.0]
      π(departments.dept_name,departments.dept_id) [rows=20, cols=2, cost=40.0]
        table(departments) [rows=20, cols=3, cost=60.0]

Breakup Table:
text
Node Type | Description                              | Node Cost | Cumulative Cost
----------|------------------------------------------|-----------|----------------
σ         | departments.dept_name = Engineering      | 1000.0    | 42060.9
π         | employees.name,departments.dept_name     | 20000.0   | 40058.0
⨝         | employees.dept_id = departments.dept_id  | 20000.0   | 40058.0
π         | employees.name,employees.dept_id         | 20000.0   | 38000.0
table     | employees                                | 40000.0   | 40000.0
π         | departments.dept_name,departments.dept_id| 40.0      | 58.0
table     | departments                              | 60.0      | 60.0

Math Explanation:

    Table (departments):
        Node Cost: 20 * 3 = 60.0
        Cumulative Cost: 60.0
        Result: Cumulative Cost = 60.0
    Projection (π, departments.dept_name,departments.dept_id):
        Node Cost: 20 * 2 = 40.0
        Child Cost (departments): 60.0
        Column Ratio: 2 / 3 ≈ 0.6667
        Cumulative Cost:
            Formula: child_cost * (0.9 + 0.1 * column_ratio)
            Calculation: 60.0 * (0.9 + 0.1 * 0.6667) = 60.0 * (0.9 + 0.06667) = 60.0 * 0.96667 ≈ 58.0
        Result: Cumulative Cost = 58.0
    Table (employees):
        Node Cost: 10000 * 4 = 40000.0
        Cumulative Cost: 40000.0
        Result: Cumulative Cost = 40000.0
    Projection (π, employees.name,employees.dept_id):
        Node Cost: 10000 * 2 = 20000.0
        Child Cost (employees): 40000.0
        Column Ratio: 2 / 4 = 0.5
        Cumulative Cost:
            Formula: child_cost * (0.9 + 0.1 * column_ratio)
            Calculation: 40000.0 * (0.9 + 0.1 * 0.5) = 40000.0 * (0.9 + 0.05) = 40000.0 * 0.95 = 38000.0
        Result: Cumulative Cost = 38000.0
    Join (⨝):
        Node Cost: 10000 * 2 = 20000.0
        Selectivity: 0.05
        Child Costs:
            Left (π employees): 38000.0
            Right (π departments): 58.0
        Cumulative Cost:
            Formula: left_cost + right_cost + (result_size * num_columns * 0.1)
            Calculation: 38000.0 + 58.0 + (10000 * 2 * 0.1) = 38058.0 + 2000.0 = 40058.0
        Result: Cumulative Cost = 40058.0
    Projection (π, employees.name,departments.dept_name):
        Node Cost: 10000 * 2 = 20000.0
        Child Cost (join): 40058.0
        Column Ratio: 2 / 2 = 1.0
        Cumulative Cost:
            Formula: child_cost * (0.9 + 0.1 * column_ratio)
            Calculation: 40058.0 * (0.9 + 0.1 * 1.0) = 40058.0 * (0.9 + 0.1) = 40058.0 * 1.0 = 40058.0
        Result: Cumulative Cost = 40058.0
    Selection (σ):
        Node Cost: 500 * 2 = 1000.0
        Selectivity: 0.05
        Child Cost (π): 40058.0
        Cumulative Cost:
            Formula: child_cost * (1.0 + selectivity)
            Calculation: 40058.0 * (1.0 + 0.05) = 40058.0 * 1.05 ≈ 42060.9
        Result: Cumulative Cost = 42060.9

Total Cost: The root node’s Cumulative Cost = 42060.9.

Why Higher Cost?:

    Projections reduce columns early (e.g., 2 columns vs. 7 in join), but the join still processes 10000 rows because the selection (σ) is applied last.
    Node Costs (1000.0 + 20000.0 + 20000.0 + 20000.0 + 40000.0 + 40.0 + 60.0 = 101000.0) don’t sum to 42060.9 due to column ratio reductions (0.95, 0.96667) and selectivity (0.05).

Breakup Table Insight:

    σ: Entire plan = 42060.9
    π (top): π + join subtree = 40058.0
    ⨝: Join + both π subtrees = 40058.0
    π (employees): π + employees = 38000.0
    Table (employees): 40000.0
    π (departments): π + departments = 58.0
    Table (departments): 60.0

Why Selection Pushdown is Best

    Original (45883.5): Join processes 10000 rows because selection is applied after the join, leading to a high join cost (47060.0).
    Selection Pushdown (37526.4): Selection reduces departments to 1 row before the join, lowering the join’s result to 500 rows and its cost to 40413.0.
    Projection Pushdown (42060.9): Reduces columns early but doesn’t reduce rows until the final selection, so the join still processes 10000 rows (cost 40058.0).

The Selection Pushdown plan minimizes the join’s intermediate result size, making it the most efficient.